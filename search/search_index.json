{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SecActPy","text":"<p>Secreted Protein Activity Inference using Ridge Regression</p> <p> </p> <p>Python implementation of SecAct for inferring secreted protein activities from gene expression data.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>SecAct Compatible: Matches R SecAct/RidgeR results on the same platform (<code>rng_method='srand'</code>)</li> <li>GPU Acceleration: Optional CuPy backend for large-scale analysis</li> <li>Million-Sample Scale: Batch processing with streaming output for massive datasets</li> <li>Built-in Signatures: Includes SecAct and CytoSig signature matrices</li> <li>Multi-Platform Support: Bulk RNA-seq, scRNA-seq, and Spatial Transcriptomics (Visium, CosMx)</li> <li>Smart Caching: Optional permutation table caching for faster repeated analyses</li> <li>Sparse-Aware: Automatic memory-efficient processing for sparse single-cell data</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation \u2014 install SecActPy from PyPI or GitHub</li> <li>Quick Start \u2014 run your first analysis in minutes</li> </ul>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>Batch Processing \u2014 handle large datasets with memory-efficient batching</li> <li>GPU Acceleration \u2014 speed up computation with CuPy</li> <li>Reproducibility \u2014 RNG backends for cross-platform reproducibility</li> <li>Docker \u2014 run SecActPy in Docker containers</li> <li>CLI Reference \u2014 command-line interface documentation</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<ul> <li>API Reference \u2014 full function signatures and parameters</li> <li>Advanced API \u2014 low-level <code>ridge()</code> / <code>ridge_batch()</code> usage</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use SecActPy in your research, please cite:</p> <p>Beibei Ru, Lanqi Gong, Emily Yang, Seongyong Park, George Zaki, Kenneth Aldape, Lalage Wakefield, Peng Jiang. Inference of secreted protein activities in intercellular communication. Nature Methods, 2026 (In press)</p>"},{"location":"#related-projects","title":"Related Projects","text":"<ul> <li>SecAct \u2014 Original R implementation</li> <li>RidgeR \u2014 R ridge regression package</li> <li>SpaCET \u2014 Spatial transcriptomics cell type analysis</li> <li>CytoSig \u2014 Cytokine signaling inference</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License \u2014 see LICENSE for details.</p>"},{"location":"advanced_api/","title":"Advanced API: Low-Level Functions","text":"<p>The high-level functions handle gene subsetting, scaling, centering, and streaming output automatically. If you need more control \u2014 for example, to pass a pre-processed sparse matrix directly \u2014 use the low-level functions:</p> Function Input Best for <code>ridge()</code> Dense or sparse Y (fits in memory) Small-to-medium datasets, single-call <code>ridge_batch()</code> Dense or sparse Y (any size) Large datasets, streaming output <p>Both functions support the same <code>backend</code> options (<code>'numpy'</code>, <code>'cupy'</code>, <code>'auto'</code>), <code>sparse_mode</code>, <code>col_center</code>, <code>col_scale</code>, and produce identical results for the same inputs.</p>"},{"location":"advanced_api/#dense-vs-sparse-inputs","title":"Dense vs sparse inputs","text":"<p>How normalization is handled depends on the input format:</p> <ul> <li>Dense (NumPy array): You must z-score normalize Y yourself before   calling, since neither <code>ridge()</code> nor <code>ridge_batch()</code> scans the full dense   array upfront. The <code>col_center</code>/<code>col_scale</code> flags are ignored.</li> <li>Sparse (<code>scipy.sparse</code> matrix): Column statistics are computed   efficiently from the sparse structure, then normalization is applied   in-flight. Use <code>col_center</code> and <code>col_scale</code> to control which normalization   steps are applied (both default to <code>True</code>).</li> </ul>"},{"location":"advanced_api/#column-normalization-flags-for-sparse-y","title":"Column normalization flags for sparse Y","text":"<p>When Y is sparse, the <code>col_center</code> and <code>col_scale</code> parameters control in-flight normalization.</p> <p>Notation:</p> Symbol Shape Definition T (m, p) Projection matrix: <code>(X'X + \u03bbI)\u207b\u00b9 X'</code>, where X is the signature matrix Y (p, n) Expression matrix (genes \u00d7 samples), sparse T\u1d62\u2096 scalar Element at row i, column k of T Y\u2096\u2c7c scalar Element at row k, column j of Y \u03bc\u2c7c scalar Mean of column j of Y: <code>\u03bc\u2c7c = (1/p) \u03a3\u2096 Y\u2096\u2c7c</code> \u03c3\u2c7c scalar Standard deviation of column j of Y \u03bc (n,) Vector of all column means \u03c3 (n,) Vector of all column stds \u03a3\u2096 \u2014 Summation over k = 1, \u2026, p (gene axis) <p>Formulas:</p> <code>col_center</code> <code>col_scale</code> Element (i, j) formula Python (vectorized) Description <code>True</code> <code>True</code> <code>\u03a3\u2096 T\u1d62\u2096(Y\u2096\u2c7c \u2212 \u03bc\u2c7c) / \u03c3\u2c7c</code> <code>(T @ Y - T.sum(1)[:, None] * \u03bc) / \u03c3</code> Full z-scoring (default) <code>True</code> <code>False</code> <code>\u03a3\u2096 T\u1d62\u2096(Y\u2096\u2c7c \u2212 \u03bc\u2c7c)</code> <code>T @ Y - T.sum(1)[:, None] * \u03bc</code> Mean-center only <code>False</code> <code>True</code> <code>\u03a3\u2096 T\u1d62\u2096 Y\u2096\u2c7c / \u03c3\u2c7c</code> <code>T @ Y / \u03c3</code> Scale only <code>False</code> <code>False</code> <code>\u03a3\u2096 T\u1d62\u2096 Y\u2096\u2c7c</code> <code>T @ Y</code> Raw projection <p>Broadcasting: <code>\u03bc</code> and <code>\u03c3</code> are 1-D vectors of length n (one value per column of Y). <code>T @ Y</code> produces an (m \u00d7 n) matrix and the vector operations broadcast across it:</p> <ul> <li><code>/ \u03c3</code> \u2014 <code>\u03c3</code> has shape <code>(n,)</code>. NumPy broadcasts it as a row vector,   dividing each column j of the matrix by <code>\u03c3\u2c7c</code>. This scales every element   in column j by the same scalar.</li> <li><code>* \u03bc</code> \u2014 same broadcasting: <code>\u03bc</code> has shape <code>(n,)</code> and multiplies each   column j by <code>\u03bc\u2c7c</code>.</li> <li><code>T.sum(1)[:, None]</code> \u2014 row sums of T, shape <code>(m, 1)</code>. The <code>[:, None]</code>   reshapes it into a column vector so that <code>T.sum(1)[:, None] * \u03bc</code> produces   an (m \u00d7 n) outer product, where element (i, j) equals <code>(\u03a3\u2096 T\u1d62\u2096) \u00b7 \u03bc\u2c7c</code>.   This is the centering correction term subtracted from <code>T @ Y</code>.</li> </ul> <p>These flags work with both <code>ridge()</code> and <code>ridge_batch()</code>, and with both <code>sparse_mode=True</code> and <code>sparse_mode=False</code>.</p>"},{"location":"advanced_api/#examples","title":"Examples","text":"<pre><code>from secactpy import ridge, ridge_batch\n\n# --- ridge() with sparse Y ---\n# Full in-flight z-scoring (matches pre-scaled dense input)\nresult = ridge(X, Y_sparse, sparse_mode=True)\n\n# Raw projection without normalization\nresult = ridge(X, Y_sparse, sparse_mode=True,\n               col_center=False, col_scale=False)\n\n# --- ridge_batch() with sparse Y ---\n# Sparse end-to-end with in-flight normalization and row centering\nresult = ridge_batch(\n    X, Y_sparse,\n    batch_size=5000,\n    sparse_mode=True,    # keep Y sparse during T @ Y\n    row_center=True,     # apply row-mean centering in-flight\n    col_center=True,     # subtract column means (default)\n    col_scale=True       # divide by column stds (default)\n)\n\n# Raw sparse projection (no column normalization)\nresult = ridge_batch(\n    X, Y_sparse,\n    batch_size=5000,\n    col_center=False, col_scale=False\n)\n</code></pre>"},{"location":"advanced_api/#disabling-automatic-sparse-scaling","title":"Disabling automatic sparse scaling","text":"<p>If you do not want automatic sparse scaling, you can either set <code>col_center=False, col_scale=False</code>, or convert to dense and normalize however you like:</p> <pre><code>from secactpy import ridge_batch\n\n# Option 1: Disable in-flight normalization\nresult = ridge_batch(X, Y_sparse, batch_size=5000,\n                     col_center=False, col_scale=False)\n\n# Option 2: Convert to dense, apply your own processing\nY_dense = Y_sparse.toarray().astype(np.float64)\n# ... apply your own normalization (or skip it) ...\nresult = ridge_batch(X, Y_dense, batch_size=5000)\n</code></pre>"},{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/#high-level-functions","title":"High-Level Functions","text":"Function Description <code>secact_activity_inference()</code> Bulk RNA-seq inference <code>secact_activity_inference_scrnaseq()</code> scRNA-seq inference <code>secact_activity_inference_st()</code> Spatial transcriptomics inference <code>load_signature(name='secact')</code> Load built-in signature matrix"},{"location":"api_reference/#core-functions","title":"Core Functions","text":"Function Description <code>ridge()</code> Single-call ridge regression with permutation testing <code>ridge_batch()</code> Batch processing for large datasets (dense or sparse) <code>estimate_batch_size()</code> Estimate optimal batch size for available memory <code>estimate_memory()</code> Estimate memory requirements"},{"location":"api_reference/#key-parameters","title":"Key Parameters","text":"Parameter Default Description <code>sig_matrix</code> <code>\"secact\"</code> Signature: \"secact\", \"cytosig\", or DataFrame <code>lambda_</code> <code>5e5</code> Ridge regularization parameter <code>n_rand</code> <code>1000</code> Number of permutations <code>seed</code> <code>0</code> Random seed for reproducibility <code>rng_method</code> <code>None</code> RNG backend: <code>'srand'</code> (match R, default), <code>'gsl'</code> (cross-platform), <code>'numpy'</code> (fast). <code>None</code> defaults to srand. <code>is_group_sig</code> <code>True</code> Group similar signatures by correlation before regression <code>backend</code> <code>'auto'</code> 'auto', 'numpy', or 'cupy' <code>use_cache</code> <code>False</code> Cache permutation tables to disk <code>sparse_mode</code> <code>False</code> Keep sparse Y in sparse format (avoids densification) <code>col_center</code> <code>True</code> Subtract column means during sparse in-flight normalization <code>col_scale</code> <code>True</code> Divide by column stds during sparse in-flight normalization"},{"location":"api_reference/#st-specific-parameters","title":"ST-Specific Parameters","text":"Parameter Default Description <code>cell_type_col</code> <code>None</code> Column in AnnData.obs for cell type <code>is_spot_level</code> <code>True</code> If False, aggregate by cell type <code>scale_factor</code> <code>1e5</code> Normalization scale factor"},{"location":"api_reference/#batch-processing-parameters","title":"Batch Processing Parameters","text":"Parameter Default Description <code>batch_size</code> <code>None</code> Samples per batch (<code>None</code> = all at once) <code>output_path</code> <code>None</code> Stream results to H5AD file (requires <code>batch_size</code>) <code>output_compression</code> <code>\"gzip\"</code> Compression: \"gzip\", \"lzf\", or None <p>For low-level <code>ridge()</code> / <code>ridge_batch()</code> usage and sparse column normalization details, see Advanced API.</p>"},{"location":"batch_processing/","title":"Batch Processing","text":""},{"location":"batch_processing/#what-is-batch-processing","title":"What is batch processing?","text":"<p>By default, SecActPy loads the entire expression matrix into memory and runs ridge regression on all samples at once. This works well for most datasets, but for large-scale analyses (e.g., 50,000+ single cells or spatial spots) the memory required for permutation testing can exceed available RAM or GPU memory.</p> <p>Batch processing splits the work into smaller pieces. The expensive projection matrix <code>T = (X'X + \u03bbI)^{-1} X'</code> is computed once from the signature, then samples are processed in chunks of <code>batch_size</code> at a time. Each chunk goes through the full permutation-testing pipeline independently, and partial results are concatenated at the end. The final output is mathematically identical to processing all samples at once \u2014 only peak memory usage is reduced.</p> <p>All three high-level functions support <code>batch_size</code> and <code>output_path</code>: - <code>secact_activity_inference()</code> \u2014 bulk RNA-seq - <code>secact_activity_inference_scrnaseq()</code> \u2014 scRNA-seq - <code>secact_activity_inference_st()</code> \u2014 spatial transcriptomics</p> <p>Set <code>batch_size</code> to enable it:</p> <pre><code># Without batch processing: all samples at once (default)\nresult = secact_activity_inference(expr_df, ...)\n\n# With batch processing: 5000 samples per chunk\nresult = secact_activity_inference(expr_df, ..., batch_size=5000)\n\n# Works the same way for scRNA-seq and ST:\nresult = secact_activity_inference_scrnaseq(adata, ..., batch_size=5000)\nresult = secact_activity_inference_st(adata, ..., batch_size=5000)\n</code></pre>"},{"location":"batch_processing/#in-memory-vs-streaming-output","title":"In-memory vs streaming output","text":"<p>By default, batch results are accumulated in memory and returned as a dictionary of DataFrames \u2014 this is the in-memory mode. You get back a <code>dict</code> with <code>result['zscore']</code>, <code>result['pvalue']</code>, etc., just like the non-batched case.</p> <p>For very large datasets, even the output matrices (beta, zscore, pvalue, se \u2014 each of shape n_proteins \u00d7 n_samples) may not fit in memory. Streaming output solves this: set <code>output_path</code> to write each batch's results directly to an HDF5 file on disk as it completes. The function returns <code>None</code> in this mode \u2014 no results are held in memory. You load them back from the file when needed. All three high-level functions support this.</p> Mode Parameter Return value Memory for output In-memory (default) <code>output_path=None</code> <code>dict</code> of DataFrames All results in RAM Streaming <code>output_path=\"results.h5ad\"</code> <code>None</code> Only one batch at a time <pre><code># Streaming works with any high-level function:\nsecact_activity_inference(..., batch_size=5000, output_path=\"bulk_results.h5ad\")\nsecact_activity_inference_scrnaseq(..., batch_size=5000, output_path=\"sc_results.h5ad\")\nsecact_activity_inference_st(..., batch_size=5000, output_path=\"st_results.h5ad\")\n</code></pre>"},{"location":"batch_processing/#example-batch-processing-with-secact_activity_inference","title":"Example: batch processing with <code>secact_activity_inference</code>","text":"<p><code>secact_activity_inference</code> handles gene subsetting, z-score normalization, signature grouping, and row expansion automatically \u2014 you just pass your expression data and set <code>batch_size</code>.</p> <pre><code># Download example data (788 OV CD4 T cells, 34 MB)\nwget https://zenodo.org/records/18520356/files/OV_scRNAseq_CD4.h5ad\n</code></pre> <pre><code>from secactpy import secact_activity_inference\nimport anndata as ad\n\n# Load multi-sample expression data\nadata = ad.read_h5ad(\"OV_scRNAseq_CD4.h5ad\")\n\n# --- In-memory mode (default) ---\n# Results are returned as a dict of DataFrames\nresult = secact_activity_inference(\n    adata.to_df().T,         # genes \u00d7 cells DataFrame\n    is_differential=False,   # center by row means across samples\n    batch_size=200,          # process 200 cells per batch\n    verbose=True\n)\nprint(result['zscore'].head())  # (proteins \u00d7 cells) DataFrame\n\n# --- Streaming mode ---\n# Results are written to disk; function returns None\nsecact_activity_inference(\n    adata.to_df().T,\n    is_differential=False,\n    batch_size=200,\n    output_path=\"results.h5ad\",       # write here instead of returning\n    output_compression=\"gzip\",        # compress on disk (default)\n    verbose=True\n)\n# Load results back when needed:\nimport h5py\nwith h5py.File(\"results.h5ad\", \"r\") as f:\n    zscore = f['zscore'][:]           # NumPy array (proteins \u00d7 cells)\n</code></pre>"},{"location":"batch_processing/#sparse-mode-for-memory-efficient-processing","title":"Sparse mode for memory-efficient processing","text":"<p>By default, sparse Y matrices are converted to dense before matrix multiplication. For very large, highly sparse datasets (e.g., scRNA-seq with 100k+ cells at &lt;5% density), this can require hundreds of GB of RAM.</p> <p>Setting <code>sparse_mode=True</code> keeps Y sparse throughout the entire pipeline. Instead of densifying Y and computing <code>T @ Y_dense</code>, it uses the algebraic identity <code>(Y.T @ T.T).T</code> to perform the multiplication with Y in sparse format. Column z-scoring and row-mean centering are applied as lightweight corrections on the small output matrix, never on Y itself.</p> <p>All three high-level functions support <code>sparse_mode</code>:</p> <pre><code># scRNA-seq: sparse CPM \u2192 log2 \u2192 ridge, Y never densified\nresult = secact_activity_inference_scrnaseq(\n    adata,\n    cell_type_col=\"cell_type\",\n    is_single_cell_level=True,\n    batch_size=5000,\n    sparse_mode=True,   # keep Y sparse end-to-end\n    verbose=True\n)\n\n# Spatial transcriptomics: same sparse pipeline\nresult = secact_activity_inference_st(\n    adata,\n    batch_size=5000,\n    sparse_mode=True,\n    verbose=True\n)\n</code></pre> <p>When <code>sparse_mode=True</code>, the scrnaseq and ST functions bypass the standard dense normalization pipeline. CPM normalization and log2 transform are applied directly on the sparse matrix (both are zero-preserving), and row-mean centering + column z-scoring are handled in-flight by <code>ridge_batch</code>.</p> Default (<code>sparse_mode=False</code>) <code>sparse_mode=True</code> Memory Allocates full dense Y Y stays sparse Speed (&lt;5% density) Baseline ~1.8x faster Speed (5-10% density) Baseline ~25% slower Results Identical Identical"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to SecActPy will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#022-2026-02-08","title":"[0.2.2] - 2026-02-08","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li><code>sparse_mode=True</code> parameter in <code>ridge()</code>, <code>ridge_batch()</code>, and all   high-level inference functions for memory-efficient processing of sparse Y   matrices. Uses <code>(Y.T @ T.T).T</code> to compute <code>T @ Y</code> without densifying Y,   with column z-scoring applied as corrections on the small output matrix.</li> <li>End-to-end sparse pipeline in <code>secact_activity_inference_scrnaseq()</code> and   <code>secact_activity_inference_st()</code>: when <code>sparse_mode=True</code>, CPM normalization   and log2 transform are applied directly on sparse matrices (both are   zero-preserving), bypassing the dense <code>secact_activity_inference()</code> path.</li> <li><code>row_center=True</code> parameter in <code>ridge_batch()</code> for in-flight row-mean   centering without densifying Y. Computes row-centered column statistics   from sparse Y analytically and applies <code>T @ row_means</code> correction per   permutation.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li><code>from .ridge import ridge_batch</code> in <code>inference.py</code> -- <code>ridge_batch</code> is   defined in <code>batch.py</code>, not <code>ridge.py</code>. This caused <code>ImportError</code> when   calling <code>secact_activity_inference()</code> or <code>secact_activity_inference_st()</code>   with <code>batch_size</code> set.</li> </ul>"},{"location":"changelog/#021-2026-02-08","title":"0.2.1 - 2026-02-08","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Streaming output (<code>output_path</code>, <code>output_compression</code>) in all high-level   inference functions: <code>secact_activity_inference()</code>,   <code>secact_activity_inference_scrnaseq()</code>, and <code>secact_activity_inference_st()</code></li> <li><code>use_gsl_rng</code> parameter in <code>ridge_batch()</code> -- enables the ~70x faster NumPy   RNG path for batch processing (previously hardcoded to GSL RNG)</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li><code>use_gsl_rng</code> was accepted by <code>secact_activity_inference</code> but silently   ignored by <code>ridge_batch</code>, which always used the slower GSL RNG. Now   <code>ridge_batch</code> (both dense and sparse paths) respects the flag.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Expanded README batch processing documentation: explains what batch   processing is, in-memory vs streaming modes, dense vs sparse handling,   and includes downloadable example data</li> </ul>"},{"location":"changelog/#020-2025-01-06","title":"0.2.0 - 2025-01-06","text":""},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Official Release: Migrated to <code>data2intelligence</code></li> <li>PyPI Package: Now available via <code>pip install secactpy</code></li> <li>Updated all documentation and URLs to point to official repository</li> <li>Docker images now published to <code>psychemistz/secactpy</code></li> </ul>"},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Comprehensive CI/CD pipeline with GitHub Actions</li> <li>Automated PyPI publishing on releases</li> <li>Automated Docker image builds (CPU, GPU, with-R variants)</li> <li>Enhanced test suite covering all major functionality</li> </ul>"},{"location":"changelog/#012-2024-12-xx","title":"0.1.2 - 2024-12-XX","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Ridge regression with permutation-based significance testing</li> <li>GPU acceleration via CuPy backend (9-34x speedup)</li> <li>Batch processing with streaming H5AD output for million-sample datasets</li> <li>Automatic sparse matrix handling in <code>ridge_batch()</code></li> <li>Built-in SecAct and CytoSig signature matrices</li> <li>GSL-compatible RNG for R/RidgeR reproducibility</li> <li>Support for Bulk RNA-seq, scRNA-seq, and Spatial Transcriptomics</li> <li>Cell type resolution for ST data (<code>cell_type_col</code>, <code>is_spot_level</code>)</li> <li>Optional permutation table caching (<code>use_cache</code>)</li> <li>Command-line interface for common workflows</li> <li>Docker support with CPU, GPU, and R variants</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>High-Level API:</li> <li><code>secact_activity_inference()</code> - Bulk RNA-seq inference</li> <li><code>secact_activity_inference_st()</code> - Spatial transcriptomics inference</li> <li> <p><code>secact_activity_inference_scrnaseq()</code> - scRNA-seq inference</p> </li> <li> <p>Core API:</p> </li> <li><code>ridge()</code> - Single-call ridge regression</li> <li><code>ridge_batch()</code> - Batch processing for large datasets</li> <li> <p><code>load_signature()</code> - Load built-in signature matrices</p> </li> <li> <p>Performance:</p> </li> <li>GPU acceleration achieving 9-34x speedup</li> <li>Memory-efficient sparse matrix processing</li> <li> <p>Streaming output for very large datasets</p> </li> <li> <p>Compatibility:</p> </li> <li>Produces identical results to R SecAct/RidgeR</li> <li>GSL-compatible random number generator</li> <li>Cross-platform support (Linux, macOS, Windows)</li> </ul>"},{"location":"cli/","title":"Command Line Interface","text":"<p>SecActPy provides a command line interface for common workflows.</p>"},{"location":"cli/#commands","title":"Commands","text":"<pre><code># Bulk RNA-seq (differential expression)\nsecactpy bulk -i diff_expr.tsv -o results.h5ad --differential -v\n\n# Bulk RNA-seq (raw counts)\nsecactpy bulk -i counts.tsv -o results.h5ad -v\n\n# scRNA-seq with cell type aggregation\nsecactpy scrnaseq -i data.h5ad -o results.h5ad --cell-type-col celltype -v\n\n# scRNA-seq at single cell level\nsecactpy scrnaseq -i data.h5ad -o results.h5ad --single-cell -v\n\n# Visium spatial transcriptomics\nsecactpy visium -i /path/to/visium/ -o results.h5ad -v\n\n# CosMx (single-cell spatial)\nsecactpy cosmx -i cosmx.h5ad -o results.h5ad --batch-size 50000 -v\n\n# Use GPU acceleration\nsecactpy bulk -i data.tsv -o results.h5ad --backend cupy -v\n\n# Use CytoSig signature\nsecactpy bulk -i data.tsv -o results.h5ad --signature cytosig -v\n</code></pre>"},{"location":"cli/#cli-options","title":"CLI Options","text":"Option Description <code>-i, --input</code> Input file or directory <code>-o, --output</code> Output H5AD file <code>-s, --signature</code> Signature matrix (secact, cytosig) <code>--lambda</code> Ridge regularization (default: 5e5) <code>-n, --n-rand</code> Number of permutations (default: 1000) <code>--backend</code> Computation backend (auto, numpy, cupy) <code>--batch-size</code> Batch size for large datasets <code>-v, --verbose</code> Verbose output"},{"location":"docker/","title":"Docker Usage Guide","text":"<p>This guide explains how to use SecActPy with Docker, including CPU, GPU, and R-enabled versions.</p>"},{"location":"docker/#quick-start","title":"Quick Start","text":""},{"location":"docker/#pull-pre-built-images","title":"Pull Pre-built Images","text":"<pre><code># CPU version (Python only) - recommended for most users\ndocker pull psychemistz/secactpy:latest\n\n# GPU version (Python + CuPy)\ndocker pull psychemistz/secactpy:gpu\n\n# CPU + R version (Python + SecAct/RidgeR/SpaCET)\ndocker pull psychemistz/secactpy:with-r\n\n# GPU + R version (full stack)\ndocker pull psychemistz/secactpy:gpu-with-r\n</code></pre>"},{"location":"docker/#build-images-locally","title":"Build Images Locally","text":"<pre><code># CPU version (default, Python only ~5 min)\ndocker build -t secactpy:latest .\n\n# GPU version (Python + CuPy ~10 min)\ndocker build -t secactpy:gpu --build-arg USE_GPU=true .\n\n# CPU + R version (Python + R packages ~30-60 min)\ndocker build -t secactpy:with-r --build-arg INSTALL_R=true .\n\n# GPU + R version (full stack ~40-90 min)\ndocker build -t secactpy:gpu-with-r --build-arg USE_GPU=true --build-arg INSTALL_R=true .\n</code></pre>"},{"location":"docker/#build-arguments","title":"Build Arguments","text":"Argument Default Description <code>USE_GPU</code> <code>false</code> Set to <code>true</code> for GPU/CUDA support <code>INSTALL_R</code> <code>false</code> Set to <code>true</code> to include R and packages"},{"location":"docker/#available-docker-images","title":"Available Docker Images","text":"Tag Python GPU R Size Use Case <code>latest</code> / <code>cpu</code> Yes No No ~2 GB General use <code>gpu</code> Yes Yes No ~6 GB GPU acceleration <code>with-r</code> / <code>cpu-with-r</code> Yes No Yes ~3.5 GB R cross-validation <code>gpu-with-r</code> Yes Yes Yes ~8 GB Full stack"},{"location":"docker/#r-packages-included","title":"R Packages Included","text":"<p>When building with <code>INSTALL_R=true</code>, the following packages are installed. CRAN packages are fetched as pre-compiled binaries from the Posit Public Package Manager for faster builds.</p>"},{"location":"docker/#from-cran-pre-compiled-binaries","title":"From CRAN (pre-compiled binaries)","text":"<p>Core: - remotes, BiocManager, devtools - Matrix, Rcpp, RcppArmadillo, RcppEigen - ggplot2, dplyr, tidyr, data.table, httr, jsonlite, R6, crayon</p> <p>SecAct dependencies: - reshape2, patchwork, NMF, akima, gganimate, metap - circlize, ggalluvial, networkD3, survival, survminer - ggpubr, car, lme4, sp</p> <p>SpaCET dependencies: - scatterpie, png, shiny, plotly, DT - factoextra, NbClust, cluster, pbmcapply, psych - arrow, RANN, sctransform</p>"},{"location":"docker/#from-bioconductor-biocmanagerinstall","title":"From Bioconductor (<code>BiocManager::install()</code>)","text":"<ul> <li>Biobase, S4Vectors, IRanges</li> <li>SummarizedExperiment, SingleCellExperiment, rhdf5</li> <li>ComplexHeatmap, limma, UCell, BiRewire</li> </ul>"},{"location":"docker/#from-github-remotesinstall_github","title":"From GitHub (<code>remotes::install_github()</code>)","text":"<ul> <li>RidgeR: <code>beibeiru/RidgeR</code> -- Ridge regression with GSL RNG</li> <li>SecAct: <code>data2intelligence/SecAct</code> -- Secreted protein activity inference</li> <li>SpaCET: <code>data2intelligence/SpaCET</code> -- Spatial transcriptomics cell type analysis</li> <li>MUDAN: <code>JEFworks/MUDAN</code> -- Multi-scale Diffusion for Uniform Manifold Approximation (SpaCET dependency)</li> </ul>"},{"location":"docker/#running-containers","title":"Running Containers","text":""},{"location":"docker/#interactive-mode","title":"Interactive Mode","text":"<pre><code># CPU\ndocker run -it --rm -v $(pwd):/workspace psychemistz/secactpy:latest\n\n# GPU\ndocker run -it --rm --gpus all -v $(pwd):/workspace psychemistz/secactpy:gpu\n\n# CPU + R (for cross-validation)\ndocker run -it --rm -v $(pwd):/workspace psychemistz/secactpy:with-r\n</code></pre>"},{"location":"docker/#run-scripts","title":"Run Scripts","text":"<pre><code># Python script\ndocker run --rm -v $(pwd):/workspace psychemistz/secactpy:latest python your_script.py\n\n# R script\ndocker run --rm -v $(pwd):/workspace psychemistz/secactpy:with-r Rscript your_script.R\n</code></pre>"},{"location":"docker/#using-docker-compose","title":"Using Docker Compose","text":"<p>Docker Compose simplifies container management:</p> <pre><code># Start CPU container\ndocker-compose up -d secactpy\n\n# Start GPU container\ndocker-compose up -d secactpy-gpu\n\n# Start CPU + R container\ndocker-compose up -d secactpy-r\n\n# Enter a running container\ndocker-compose exec secactpy bash\n\n# Stop all containers\ndocker-compose down\n</code></pre>"},{"location":"docker/#available-services","title":"Available Services","text":"Service GPU R Port Description <code>secactpy</code> No No - CPU interactive <code>secactpy-gpu</code> Yes No - GPU interactive <code>secactpy-r</code> No Yes - CPU + R interactive <code>secactpy-gpu-r</code> Yes Yes - GPU + R interactive <code>secactpy-jupyter</code> No No 8888 Jupyter Lab (CPU) <code>secactpy-jupyter-gpu</code> Yes No 8889 Jupyter Lab (GPU) <code>secactpy-jupyter-r</code> No Yes 8890 Jupyter Lab (CPU + R)"},{"location":"docker/#jupyter-lab","title":"Jupyter Lab","text":"<pre><code># Start Jupyter (CPU)\ndocker-compose up secactpy-jupyter\n# Open http://localhost:8888\n\n# Start Jupyter (GPU)\ndocker-compose up secactpy-jupyter-gpu\n# Open http://localhost:8889\n\n# Start Jupyter (CPU + R)\ndocker-compose up secactpy-jupyter-r\n# Open http://localhost:8890\n</code></pre>"},{"location":"docker/#validation-examples","title":"Validation Examples","text":""},{"location":"docker/#verify-python-installation","title":"Verify Python Installation","text":"<pre><code>docker run --rm psychemistz/secactpy:latest python -c \"\nimport secactpy\nprint(f'SecActPy {secactpy.__version__}')\nprint(f'GPU available: {secactpy.CUPY_AVAILABLE}')\n\n# Load signature\nsig = secactpy.load_signature('secact')\nprint(f'Signature: {sig.shape}')\n\"\n</code></pre>"},{"location":"docker/#verify-r-installation","title":"Verify R Installation","text":"<pre><code>docker run --rm psychemistz/secactpy:with-r Rscript -e \"\ncat('R version:', R.version.string, '\\n')\n\n# Check packages\nfor (pkg in c('SecAct', 'RidgeR', 'SpaCET', 'Biobase')) {\n    if (require(pkg, quietly = TRUE, character.only = TRUE)) {\n        cat(pkg, 'OK -', as.character(packageVersion(pkg)), '\\n')\n    } else {\n        cat(pkg, 'NOT FOUND\\n')\n    }\n}\n\"\n</code></pre>"},{"location":"docker/#cross-validation-r-vs-python","title":"Cross-Validation (R vs Python)","text":"<pre><code># Enter container with R\ndocker run -it --rm -v $(pwd):/workspace psychemistz/secactpy:with-r\n\n# Inside container - Run R inference\nRscript -e \"\nlibrary(SecAct)\ndata &lt;- read.table('your_data.txt', row.names=1, header=TRUE)\nresult &lt;- SecAct.inference.gsl(data)\nwrite.table(result\\$zscore, 'r_zscore.txt', quote=FALSE)\n\"\n\n# Run Python inference\npython -c \"\nimport pandas as pd\nfrom secactpy import secact_activity_inference\n\ndata = pd.read_csv('your_data.txt', sep='\\t', index_col=0)\nresult = secact_activity_inference(data, is_differential=True)\nresult['zscore'].to_csv('py_zscore.txt', sep='\\t')\n\"\n\n# Compare results\npython -c \"\nimport pandas as pd\nimport numpy as np\n\nr_result = pd.read_csv('r_zscore.txt', sep='\\t', index_col=0)\npy_result = pd.read_csv('py_zscore.txt', sep='\\t', index_col=0)\n\ndiff = np.abs(r_result.values - py_result.values)\nprint(f'Max difference: {diff.max():.2e}')\nprint(f'Mean difference: {diff.mean():.2e}')\n\"\n</code></pre>"},{"location":"docker/#gpu-support","title":"GPU Support","text":""},{"location":"docker/#prerequisites","title":"Prerequisites","text":"<ol> <li>NVIDIA GPU with CUDA support</li> <li>NVIDIA Container Toolkit</li> </ol>"},{"location":"docker/#installation-ubuntu","title":"Installation (Ubuntu)","text":"<pre><code># Add NVIDIA repository\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\\n  sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\n# Install\nsudo apt-get update\nsudo apt-get install -y nvidia-container-toolkit\n\n# Restart Docker\nsudo systemctl restart docker\n</code></pre>"},{"location":"docker/#verify-gpu-access","title":"Verify GPU Access","text":"<pre><code># Check GPU is accessible\ndocker run --rm --gpus all psychemistz/secactpy:gpu nvidia-smi\n\n# Check CuPy\ndocker run --rm --gpus all psychemistz/secactpy:gpu python -c \"\nimport cupy as cp\nprint(f'CuPy version: {cp.__version__}')\nx = cp.arange(10)\nprint(f'GPU array: {x}')\n\"\n</code></pre>"},{"location":"docker/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Performance comparison using Ly86 dataset (16,325 genes \u00d7 100 samples, 1,000 permutations).</p>"},{"location":"docker/#r-backends-ridger","title":"R Backends (RidgeR)","text":"Backend macOS Native Docker (BLAS=1) Docker (BLAS=8) gsl.old 31.2s 592.5s 417.8s Yrow.st 26.3s 592.2s 428.5s Tcol.st 81.2s 636.3s 503.1s Tcol.mt(8) 37.6s 384.3s 466.3s Yrow.mt(8) 25.6s 501.1s 386.8s"},{"location":"docker/#python-secactpy","title":"Python (SecActPy)","text":"Environment srand gsl macOS Native 52.6s 56.8s Docker (BLAS=1) 386.1s 387.0s"},{"location":"docker/#best-elapsed-time-per-environment","title":"Best Elapsed Time per Environment","text":"Environment Best R Best Python macOS native 25.6s (Yrow.mt) 52.6s (srand) Docker 384.3s (Tcol.mt) 386.1s (srand)"},{"location":"docker/#notes","title":"Notes","text":"<ul> <li>macOS native uses Apple vecLib/Accelerate with AMX coprocessor (dedicated matrix multiplication hardware), explaining the ~15x speed advantage over Docker</li> <li>Docker on macOS uses generic OpenBLAS with basic ARM NEON SIMD (no Apple Silicon optimization)</li> <li>Docker on native Linux servers (x86_64) has &lt;1% overhead vs native \u2014 both use the same OpenBLAS/MKL, so the macOS-specific slowdown does not apply</li> <li>All backends produce numerically identical z-scores (max|diff| &lt; 2e-14)</li> </ul>"},{"location":"docker/#optimal-docker-configuration","title":"Optimal Docker Configuration","text":"<p>For best Docker performance, set <code>OPENBLAS_NUM_THREADS=1</code> and use multi-threaded RidgeR backends:</p> <pre><code>docker run --rm -e OPENBLAS_NUM_THREADS=1 \\\n  -v $(pwd):/workspace psychemistz/secactpy:with-r \\\n  Rscript -e \"library(RidgeR); SecAct.inference.Tcol.mt(expr, ncores=8)\"\n</code></pre> <p>This avoids thread oversubscription (OpenBLAS threads \u00d7 OpenMP threads &gt; CPU cores) which can cause severe performance degradation.</p>"},{"location":"docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docker/#r-package-installation-failures","title":"R Package Installation Failures","text":"<p>If R packages fail to install, try building with verbose output:</p> <pre><code>docker build -t secactpy:with-r --build-arg INSTALL_R=true --progress=plain .\n</code></pre> <p>Common issues:</p> <ul> <li>Network timeout: Increase timeout in Dockerfile or retry</li> <li>Missing system library: Add to apt-get install list</li> <li>GitHub rate limit: Wait and retry, or use a GitHub token</li> </ul>"},{"location":"docker/#permission-issues","title":"Permission Issues","text":"<pre><code># Run as current user\ndocker run -it --rm -u $(id -u):$(id -g) -v $(pwd):/workspace psychemistz/secactpy:latest\n</code></pre>"},{"location":"docker/#memory-issues","title":"Memory Issues","text":"<pre><code># Increase memory limit\ndocker run -it --rm -m 16g -v $(pwd):/workspace psychemistz/secactpy:latest\n</code></pre>"},{"location":"docker/#gpu-not-detected","title":"GPU Not Detected","text":"<pre><code># Check NVIDIA driver\nnvidia-smi\n\n# Check Docker GPU support\ndocker info | grep -i gpu\n\n# Verify nvidia-container-toolkit\nwhich nvidia-container-toolkit\n</code></pre>"},{"location":"docker/#customization","title":"Customization","text":""},{"location":"docker/#add-your-own-packages","title":"Add Your Own Packages","text":"<p>Create a custom Dockerfile:</p> <pre><code>FROM psychemistz/secactpy:with-r\n\n# Add R packages\nRUN R -e \"install.packages('your_r_package', repos='https://cloud.r-project.org/')\"\nRUN R -e \"BiocManager::install('bioc_package')\"\nRUN R -e \"remotes::install_github('user/repo')\"\n\n# Add Python packages\nRUN pip3 install your_python_package\n</code></pre>"},{"location":"docker/#mount-additional-data","title":"Mount Additional Data","text":"<pre><code>docker run -it --rm \\\n  -v $(pwd):/workspace \\\n  -v /path/to/data:/data:ro \\\n  -v /path/to/results:/results \\\n  psychemistz/secactpy:latest\n</code></pre>"},{"location":"docker/#singularity-hpc","title":"Singularity / HPC","text":"<p>On HPC clusters where Docker is not available, use Singularity or Apptainer to pull the Docker images:</p> <pre><code># Load Singularity or Apptainer (cluster-specific)\nmodule load singularity   # or: module load apptainer\n\n# Pull CPU image\nsingularity pull docker://psychemistz/secactpy:latest\n\n# Pull R-enabled image\nsingularity pull docker://psychemistz/secactpy:with-r\n\n# Run interactively\nsingularity exec secactpy_latest.sif python3 -c \"import secactpy; print(secactpy.__version__)\"\n\n# Run with data binding\nsingularity exec --bind /path/to/data:/data secactpy_latest.sif python3 your_script.py\n\n# Run R inside the container\nsingularity exec secactpy_with-r.sif Rscript -e \"library(SecAct); library(RidgeR); cat('OK\\n')\"\n</code></pre> <p>Note</p> <p>Building images from the Dockerfile on HPC requires <code>fakeroot</code> support. If unavailable, pull the pre-built images from Docker Hub instead.</p>"},{"location":"docker/#cicd-notes","title":"CI/CD Notes","text":"<ul> <li>Default CI builds use <code>INSTALL_R=false</code> for speed</li> <li>R-enabled builds are triggered on releases or manual dispatch</li> <li>Use <code>workflow_dispatch</code> with <code>build_r=true</code> to build R images manually</li> </ul> <pre><code># Trigger R build manually in GitHub Actions\nworkflow_dispatch:\n  inputs:\n    build_r:\n      description: 'Build images with R packages'\n      default: 'true'\n</code></pre>"},{"location":"gpu_acceleration/","title":"GPU Acceleration","text":"<p>SecActPy supports GPU acceleration via CuPy for large-scale analysis.</p>"},{"location":"gpu_acceleration/#setup","title":"Setup","text":"<pre><code>from secactpy import secact_activity_inference, CUPY_AVAILABLE\n\nprint(f\"GPU available: {CUPY_AVAILABLE}\")\n\n# Auto-detect GPU\nresult = secact_activity_inference(expression, backend='auto')\n\n# Force GPU\nresult = secact_activity_inference(expression, backend='cupy')\n</code></pre>"},{"location":"gpu_acceleration/#performance-benchmarks","title":"Performance Benchmarks","text":"Dataset R (Mac M1) R (Linux) Py (CPU) Py (GPU) Speedup Bulk (1,170 sp \u00d7 1,000 samples) 74.4s 141.6s 128.8s 6.7s 11\u201319x scRNA-seq (1,170 sp \u00d7 788 cells) 54.9s 117.4s 104.8s 6.8s 8\u201315x Visium (1,170 sp \u00d7 3,404 spots) 141.7s 379.8s 381.4s 11.2s 13\u201334x CosMx (151 sp \u00d7 443,515 cells) 936.9s 976.1s 1226.7s 99.9s 9\u201312x Benchmark Environment  - **Mac CPU**: M1 Pro with VECLIB (8 cores) - **Linux CPU**: AMD EPYC 7543P (4 cores) - **Linux GPU**: NVIDIA A100-SXM4-80GB"},{"location":"gpu_acceleration/#cuda-version-notes","title":"CUDA Version Notes","text":"<pre><code># CUDA 11.x\npip install \"secactpy[gpu]\"\n\n# CUDA 12.x (do NOT use [gpu] extra)\npip install secactpy\npip install cupy-cuda12x\n</code></pre> <p>Important (CUDA 12.x users): Do not use the <code>[gpu]</code> extra on CUDA 12.x systems \u2014 it installs <code>cupy-cuda11x</code>, which conflicts with <code>cupy-cuda12x</code>. If you already installed with <code>[gpu]</code>, remove the conflicting package first: <pre><code>pip uninstall cupy-cuda11x\npip install cupy-cuda12x\n</code></pre></p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.9</li> <li>NumPy &gt;= 1.20</li> <li>Pandas &gt;= 1.3</li> <li>SciPy &gt;= 1.7</li> <li>h5py &gt;= 3.0</li> <li>anndata &gt;= 0.8</li> <li>scanpy &gt;= 1.9</li> </ul> <p>Optional: CuPy &gt;= 10.0 (GPU acceleration)</p> <p>Virtual Environment</p> <p>Create a virtual environment before installing to avoid dependency conflicts:</p> <pre><code>python -m venv secactpy-env\nsource secactpy-env/bin/activate   # Linux/macOS\n# secactpy-env\\Scripts\\activate    # Windows\n</code></pre>"},{"location":"installation/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<pre><code># CPU Only\npip install secactpy\n\n# With GPU Support (CUDA 11.x)\npip install \"secactpy[gpu]\"\n\n# With GPU Support (CUDA 12.x)\npip install secactpy\npip install cupy-cuda12x\n</code></pre>"},{"location":"installation/#from-github","title":"From GitHub","text":"<pre><code># CPU Only\npip install git+https://github.com/data2intelligence/SecActpy.git\n\n# With GPU Support (CUDA 11.x)\npip install \"secactpy[gpu] @ git+https://github.com/data2intelligence/SecActpy.git\"\n\n# With GPU Support (CUDA 12.x)\npip install git+https://github.com/data2intelligence/SecActpy.git\npip install cupy-cuda12x\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<pre><code>git clone https://github.com/data2intelligence/SecActpy.git\ncd SecActpy\npip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#documentation-tools","title":"Documentation Tools","text":"<p>To build the documentation site locally:</p> <pre><code>pip install -e \".[docs]\"\nmkdocs serve\n</code></pre>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#example-data","title":"Example Data","text":"<p>Example datasets for all tutorials are available on Zenodo:</p> <p></p> Example Input File Output File Size Bulk RNA-seq <code>Ly86-Fc_vs_Vehicle_logFC.txt</code> <code>Ly86-Fc_vs_Vehicle_logFC_output.h5ad</code> 0.5 MB scRNA-seq (OV CD4 T cells) <code>OV_scRNAseq_CD4.h5ad</code> <code>OV_scRNAseq_ct_CD4_output.h5ad</code>, <code>OV_scRNAseq_sc_CD4_output.h5ad</code> 34 MB Visium ST (HCC) <code>Visium_HCC_data.h5ad</code> <code>Visium_HCC_output.h5ad</code> 255 MB CosMx (LIHC) <code>LIHC_CosMx_data.h5ad</code> <code>LIHC_CosMx_output.h5ad</code> 3.0 GB <p>Download all example files:</p> <pre><code># Download individual files from Zenodo\nwget https://zenodo.org/records/18520356/files/Ly86-Fc_vs_Vehicle_logFC.txt\nwget https://zenodo.org/records/18520356/files/OV_scRNAseq_CD4.h5ad\nwget https://zenodo.org/records/18520356/files/Visium_HCC_data.h5ad\nwget https://zenodo.org/records/18520356/files/LIHC_CosMx_data.h5ad\n</code></pre>"},{"location":"quickstart/#example-1-bulk-rna-seq","title":"Example 1: Bulk RNA-seq","text":"<pre><code>import pandas as pd\nfrom secactpy import secact_activity_inference\n\n# Load differential expression data (genes x samples)\n# Download: https://zenodo.org/records/18520356/files/Ly86-Fc_vs_Vehicle_logFC.txt\ndiff_expr = pd.read_csv(\"Ly86-Fc_vs_Vehicle_logFC.txt\", sep=r\"\\s+\", index_col=0)\n\n# Run inference\nresult = secact_activity_inference(\n    diff_expr,\n    is_differential=True,\n    sig_matrix=\"secact\",  # or \"cytosig\"\n    verbose=True\n)\n\n# Access results\nactivity = result['zscore']    # Activity z-scores\npvalues = result['pvalue']     # P-values\ncoefficients = result['beta']  # Regression coefficients\n</code></pre> <p>Note</p> <p>Set <code>is_differential=True</code> when the input is already log fold-change data. For single-column input with no control, row-mean centering is automatically skipped (it would produce all zeros).</p>"},{"location":"quickstart/#example-2-scrna-seq-analysis","title":"Example 2: scRNA-seq Analysis","text":"<pre><code>import anndata as ad\nfrom secactpy import secact_activity_inference_scrnaseq\n\n# Load scRNA-seq data (788 OV CD4 T cells, 3 subtypes)\n# Download: https://zenodo.org/records/18520356/files/OV_scRNAseq_CD4.h5ad\nadata = ad.read_h5ad(\"OV_scRNAseq_CD4.h5ad\")\n\n# Pseudo-bulk by cell type\nresult = secact_activity_inference_scrnaseq(\n    adata,\n    cell_type_col=\"Annotation\",\n    is_single_cell_level=False,\n    verbose=True\n)\n\n# Single-cell level\nresult_sc = secact_activity_inference_scrnaseq(\n    adata,\n    cell_type_col=\"Annotation\",\n    is_single_cell_level=True,\n    verbose=True\n)\n</code></pre>"},{"location":"quickstart/#example-3-spatial-transcriptomics","title":"Example 3: Spatial Transcriptomics","text":""},{"location":"quickstart/#visium-spot-level","title":"Visium (spot-level)","text":"<pre><code>from secactpy import secact_activity_inference_st\n\n# Load Visium HCC data (3,415 spots)\n# Download: https://zenodo.org/records/18520356/files/Visium_HCC_data.h5ad\nresult = secact_activity_inference_st(\n    \"Visium_HCC_data.h5ad\",\n    min_genes=1000,\n    verbose=True\n)\n\nactivity = result['zscore']  # (proteins x spots)\n</code></pre>"},{"location":"quickstart/#cosmx-single-cell-spatial","title":"CosMx (single-cell spatial)","text":"<pre><code>import anndata as ad\nfrom secactpy import secact_activity_inference_st\n\n# Load CosMx LIHC data (443,515 cells, 1,000 genes, 12 cell types)\n# Download: https://zenodo.org/records/18520356/files/LIHC_CosMx_data.h5ad\nadata = ad.read_h5ad(\"LIHC_CosMx_data.h5ad\")\n\n# Single-cell resolution (one score per cell)\nresult = secact_activity_inference_st(\n    adata,\n    is_spot_level=True,         # Score each cell individually (default)\n    batch_size=5000,            # Process in chunks to limit memory\n    output_path=\"cosmx_sc_results.h5ad\",  # Stream to disk\n    verbose=True\n)\n# result is None when output_path is set; load with ad.read_h5ad()\n\n# Cell-type resolution (pseudo-bulk by cell type)\nresult = secact_activity_inference_st(\n    adata,\n    cell_type_col=\"cellType\",  # Column in adata.obs\n    is_spot_level=False,        # Aggregate by cell type\n    verbose=True\n)\n\nactivity = result['zscore']  # (proteins x cell_types)\n</code></pre>"},{"location":"quickstart/#command-line-interface","title":"Command Line Interface","text":"<p>SecActPy also provides a CLI for common workflows:</p> <pre><code>secactpy bulk -i diff_expr.tsv -o results.h5ad --differential -v\nsecactpy scrnaseq -i data.h5ad -o results.h5ad --cell-type-col celltype -v\nsecactpy visium -i /path/to/visium/ -o results.h5ad -v\nsecactpy cosmx -i cosmx.h5ad -o results.h5ad --batch-size 50000 -v\n</code></pre> <p>See CLI Reference for all commands and options.</p>"},{"location":"reproducibility/","title":"Reproducibility","text":"<p>SecActPy supports three RNG backends via the <code>rng_method</code> parameter, each offering different trade-offs between R compatibility and performance:</p> <code>rng_method</code> Description Use case <code>'srand'</code> C stdlib <code>srand()</code>/<code>rand()</code> via ctypes Match R SecAct/RidgeR results on the same platform <code>'gsl'</code> Mersenne Twister (GSL-compatible) Cross-platform reproducibility within SecActPy <code>'numpy'</code> Native NumPy RNG (~70x faster) Fast analysis when reproducibility with R is not needed"},{"location":"reproducibility/#matching-r-secactridger-output","title":"Matching R SecAct/RidgeR output","text":"<p>To reproduce R SecAct/RidgeR results on the same machine, use <code>rng_method='srand'</code>. This uses the C standard library's <code>rand()</code> function, which matches R's internal RNG on the same platform. Note that C <code>rand()</code> implementations differ across operating systems, so results are platform-dependent.</p> <pre><code>result = secact_activity_inference(\n    expression,\n    is_differential=True,\n    sig_matrix=\"secact\",\n    lambda_=5e5,\n    n_rand=1000,\n    seed=0,\n    rng_method=\"srand\",  # Match R SecAct on same platform\n)\n</code></pre>"},{"location":"reproducibility/#cross-platform-reproducibility","title":"Cross-platform reproducibility","text":"<p>The <code>rng_method='gsl'</code> backend uses a portable Mersenne Twister implementation that produces identical results across all platforms (Linux, macOS, Windows). This does not match R output, but guarantees consistent SecActPy results everywhere.</p> <pre><code>result = secact_activity_inference(\n    expression,\n    rng_method=\"gsl\",  # Cross-platform reproducible\n)\n</code></pre>"},{"location":"reproducibility/#fastest-analysis","title":"Fastest analysis","text":"<p>For maximum throughput when reproducibility is not required:</p> <pre><code>result = secact_activity_inference(\n    expression,\n    rng_method=\"numpy\",  # ~70x faster permutation generation\n)\n</code></pre>"}]}